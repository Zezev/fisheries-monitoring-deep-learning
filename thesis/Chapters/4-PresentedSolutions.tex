% Chapter Template

\chapter{Soluciones presentadas} % Main chapter title
\label{cap:soluciones} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Metodología}
\label{sec:metodology}

La puntuación final de la competición será calculada mediante una función de pérdida logarítmica, como vimos en el apartado \ref{sec:envio-y-eval}. Al no tener las categorías del conjunto de datos de test debemos delegar el cálculo de la puntuación en \textit{Kaggle}, ,que permite enviar hasta cinco predicciones al día. Esto es un obstáculo para hacer pequeñas pruebas e iterar rápido sobre los resultados. Por otra parte, el segundo conjunto de test tiene más de 13000 imágenes, que no son rápidas de cargar en memoria ni de pasar por el modelo que se genere. En determinados casos la evaluación del segundo conjunto de test ha tardado más de 6 horas.

La solución a este problema se ha resuelto usando un subconjunto del conjunto de entrenamiento dedicado solo a la evaluación de modelos. Por otra parte, para el entrenamiento es necesario usar un subconjunto de validación, por lo tanto es necesario dividir el conjunto de entrenamiento original en tres subconjuntos: entrenamiento, validación y test.

La partición se ha realizado dejando un \textbf{60 \%} de los datos al conjunto de entrenamiento, un \textbf{20 \%} al conjunto de validación y un \textbf{20 \%} al conjunto de test.


\subsection{Partición del conjuntos de datos}
La partición del conjunto de datos es una operación delicada. \textbf{Ya vimos} que uno de los problemas del conjunto de datos era que la cantidad de imágenes para cada clase era muy variada, teniendo algunas clases un número muy bajo de ejemplos. Si seleccionamos aleatoriamente el 40 \% de las imágenes, es posible que no se elijan ejemplos de una o varias de las categorías, con lo que los modelos construidos no las tendrían en cuenta.

La solución adoptada ha sido realizar un muestreo estratificado, seleccionando aleatoriamente el 20\% de los ejemplos de cada clase.

\subsection{Evaluación del modelo}

Para una búsqueda más óptima de parámetros y configuraciones el modelo se evaluará contra el grupo de test generado. Una vez se encuentren determinados modelos especialmente interesantes por tener una puntuación máxima local entre aquellos a los que se compara habrá que enviarlo a \textit{Kaggle} para su evaluación completa.

Enviar el modelo a \textit{Kaggle} se puede hacer sobre una partición más grande del conjunto de entrenamiento, ya que no es necesario usar un conjunto de test para evaluar, solo uno de validación. Esto permite entrenar el modelo sobre el 80\% de los datos consiguiendo, por lo general, resultados más precisos.



\subsection{Software}

Todos los modelos de redes convolucionales que se van a entrenar en este proyecto han sido entrenados usando Keras


\section{Idea}
A la hora de afrontar este problema de clasificación de peces es natural
seguir una estrategia separada en dos pasos: primero buscar si
existe un pez en la imágen y luego intentar clasificarlo en una de las
categorías existentes.

Para buscar un pez en una imágen es necesario encontrar una serie de
características que puedan ser identificadas con algún pez. La idea de
la solución parte de esta base. Para clasificar una imagen,
primero es necesario encontrar el contenido relevante para ser usado
en la clasificación.

Como ya se comentó al hablar de las redes convolucionales (capítulo \ref{sec:conv-net-arch}),
las arquitecturas encontradas en problemas similares \parencite{krizhevsky2012imagenet}
permiten separar con claridad estas dos etapas mediante el uso de redes convolucionales (CONV) y capas densas (FC).

\section{Arquitectura}

La competición de reconocimiento visual a gran escala de ImageNet, conocida por sus siglas ILSVRC (\textit{Imagenet Large Scale Visual Recognition Challenge}) \parencite{ILSVRC} es una competición de reconocimiento de imágenes sobre un conjunto de 150000 imágenes. Estas imágenes deben clasificarse en 1000 categorías diferentes.

En 2010 y 2011 los modelos ganadores consiguieron una tasa de error de un 28,2 \% y un 25.8 \%, respectivamente. En 2012, el modelo presentado en \parencite{krizhevsky2012imagenet} ganó con una tasa de error 16.4 \%, superando en casi un 10 \% al segundo clasificado. Esta victoria hizo que el enfoque de esta propuesta haya sido usado por los ganadores de los años siguientes.

La figura \ref{general-architecture} representa la red convolucional profunda usada en ILSVRC 2012, la cual se compone de una red convolucional para la extracción de características de la imagen seguida de una red densa para la clasificación de la imagen usando sus características extraídas.

\begin{figure}
  \caption{Arquitectura de una red convolucional profunda. Consta de una acumulación de capas convolucionales destinadas a extraer características de la entrada y una serie de capas densas para clasificar la entrada usando dichas características.}
\label{general-architecture}
  \makebox[\textwidth]{\includegraphics[width=\linewidth]{imagenet-arch}}
\end{figure}

La idea no es solo usar la arquitectura de este tipo de redes, sino también sus entrenamientos. ILSVRC debe clasificar entre 1000 categorías diferentes, por lo que necesita poder generalizar mucho a la hora de extraer características de las imágenes. Para aplicar esto al reconocimiento de peces, extraeremos las características de las imágenes y usaremos otro modelo para clasificar en nuestras ocho categorías usando estas características. De esta manera podremos aprovechar la potencia de este tipo de redes con modificaciones adaptadas a nuestro problema.

\subsection{Red convolucional}

Como explicamos en el capítulo~\ref{sec:conv-net}, cuando un modelo convolucional recibe una imagen devuelve $N$ matrices bidimensionales representando el resultado de la aplicación de los $N$ conjuntos de filtros a la imagen inicial. Al tener todas esas $N$ matrices bidimensionales las mismas dimensiones también pueden considerarse como una única matriz tridimensional.

Intuitivamente podemos pensar que la salida cada uno de estos filtros representa la aparición en la imagen de la característica que esté detectando el filtro. En los filtros más cercanos al final de la red, esta podría generar filtros que se activen sólo con la aparición de una clase de pez.

Hay que tener en cuenta que este modelo convolucional no ha sido entrenado con el conjunto de entrenamiento del problema, sino con el conjunto de entrenamiento de \parencite{imagenet}. Por lo tanto, esta red convolucional va a ser usada para transformar cada imagen del conjunto de entrenamiento del problema a un conjunto de características.

\subsection{Modelo preentrenado}

La arquitectura del artículo original \parencite{krizhevsky2012imagenet} usa capas convolucionales donde alterna filtros de $11\times11$, $5\times5$ y $3\times3$, el cual parece una buena elección para usar como modelo convolucional preentrenado. Sin embargo, la aplicación de este trabajo es una competición internacional donde se usarán soluciones \textit{state of the art}. El modelo de la figura \ref{general-architecture} representa el ganador de la edición 2012 de la competición ILSVRC, por lo que resulta conveniente analizar los modelos ganadores de años posteriores.

El modelo principal a usar es VGG, desarrollado por el \textit{Visual Geometry Group}, de la Universidad de Oxford. Es un modelo especialmente interesante por su simplicidad, aparte de obtener una de las mejores puntuaciones en ILSVRC 2014.


Una de las principales características de VGG es la idea de que los filtros convolucionales mayores de $3\times3$, como por ejemplo los de $5\times5$ u $11\times11$ pueden ser representados por combinaciones de filtros $3\times3$.

De las configuraciones descritas en \parencite{simonyan} hay una que sobresale por su eficiencia, llamada VGG16. Usando un total de trece capas CONV con filtros de $3\times3$, cinco capas POOL y tres capas FC (de 4096, 4096 y 1000 salidas), seguida de una función \textit{softmax} (figura \ref{vgg16-arch}), es capaz de mejorar la eficacia del modelo de Krizhevsky. El nombre de esta configuración es VGG16, ya que es la cantidad de capas CONV y FC que posee.

\begin{figure}
  \caption{Arquitectura de VGG}
\label{vgg16-arch}
  \makebox[\textwidth]{\includegraphics[width=.7\linewidth]{vgg16-arch}}
\end{figure}

\subsection{Ajuste fino}

Si observamos la última capa del modelo VGG16, vemos que la salida tiene 1000 elementos. Tiene esta forma ya que ILSVRC consiste en clasificar una imagen entre mil categorías diferentes. Como en nuestro problema solo tenemos ocho categorías, es lógico modificar esta última capa para que únicamente tenga ocho salidas.

Al modificar la estructura de la última capa estamos destruyendo pesos y haciendo que muchos de los que ya existían carezcan de sentido. El hecho de que VGG tenga esta separación lógica entre la red convolucional y la red densa (las tres capas FC) hace que se pueda separar el modelo en dos submodelos diferentes: uno convolucional, que no habrá cambiado con la adaptación a las ocho categorías, y otro denso, que tendrá que ser entrenado de nuevo.

Al dividir la red en dos subredes diferentes hay que tener en cuenta que la segunda, la red densa, no recibe como entrada las imágenes, sino la salida de la primera red convolucional, con todas las transformaciones que esta produce. Es necesario entonces aplicar la red convolucional a todo el conjunto de datos para crear un nuevo conjunto de datos con el que reentrenar la red densa.

Esta técnica de ajustar los parámetros de un modelo ya conocido para adaptarlo a un nuevo conjunto de datos se conoce como ajuste fino (\textit{fine-tuning}).

Usando el conjunto de datos definido en el capítulo \ref{sec:metodology}, con la separación 60 \%, 20 \% y 20 \% para los conjuntos de entrenamiento, validación y test se ha entrenado la red densa.

Primero es necesaria la transformación del conjunto en las características producidas por la red convolucional.

\begin{python}
# Carga del modelo VGG16
from vgg16 import Vgg16
model = Vgg16()

# Diferencia entre red convolucional y densa
import utils
conv_model, dense_model = utils.split_at(model, MaxPooling2D)

# Cargamos los diferentes conjuntos de datos
train, train_labels = get_data(path + 'train')
valid, valid_labels = get_data(path + 'valid')
test, test_labels = get_data(path + 'test')

# Convertir dataset a features mediante la red convolucional
train_feat = conv_model.predict(train)
valid_feat = conv_model.predict(valid)
test_feat = conv_model.predict(test)
\end{python}

Este código no es completamente válido, ya que se han tomado algunas libertades para mejorar la comprensión y lectura del mismo. Define una estructura general a seguir para este problema (carga de datos, transformación, \textit{fine-tuning} y evaluación). Para una mayor comprensión el la mayor parte de las librerías usadas así como las utilidades están descritas en el Apéndice 1.

TODO (cuando lo pase a LaTex)

Una vez cargadas las librerías necesarias (Vgg16 es una clase abstracta que importa las bibliotecas necesarias de Keras), ambos modelos y transformado el dataset mediante la red convolucional se puede pasar a reentrenar la última parte de la red. Primero hay que cambiar el final de la red para conseguir que tenga ocho salidas y luego entrenar.


\begin{python}

# Cambiar el final de la red densa de 1000 outputs a 8
dense_model.pop()  # Elimina la capa final
dense_model.add(
    Dense(8, activation='softmax')  # Introducir una nueva capa
)

# Compilar el modelo
dense_model.compile(
    SGD(lr=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy'],  # Muestra la precision del modelo
)

# Entrenar la red
dense_model.fit(
    train_feat,
    train_labels,
    batch_size=64,  # Numero de imagenes a entrenar al mismo tiempo
    nb_epoch=7,     # Numero de iteraciones del entrenamiento
    validation_data=(valid_feat, valid_labels),
)

# Evaluar el modelo sobre el conjunto de test
dense_model.evaluate(test_feat, test_labels)
# Log loss, accuracy
>>> [2.38985158622892523, 0.66099843993759748]
\end{python}

La puntuación total de \textit{kaggle} para este conjunto de test sería $2.389$. Esto es sobre un conjunto de test muy pequeño, solo el 20 \% del conjunto de entrenamiento. Como se ya ha dicho anteriormente, habría que subir las predicciones del conjunto de test final a \textit{kaggle} para verificar la puntuación del modelo, pero debido a las restricciones comentadas mediremos de esta forma los modelos generados.

El segundo número que devuelve la función $evaluate$ es la precisión ($accuracy$) del modelo. Al ser el modelo un modelo de clasificación, el usar $"accuracy"$ como métrica al compilar el modelo hace devuelva el porcentaje de veces que la clase con la probabilidad máxima se corresponde con la clase etiquetada en el conjunto de datos. En este caso el modelo ha clasificado correctamente el 66 \% de las imágenes, un número muy aceptable teniendo en cuenta que solo se ha tocado la capa final de un modelo ajeno.

\section{Modelo completamente conectado}

La idea original que se comentaba al principio consistía en separar el problema en el modelo convolucional y el modelo de clasificación. Es en el segundo donde se puede conseguir toda la flexibilidad.

El modelo original de VGG usa dos capas $densas$ de 4096 neuronas para clasificar entre mil clases diferentes. Ya que este problema tiene solo ocho clases diferentes probablemente no sea necesario usar capas tan grandes, por lo que se procede a probar la misma estructura original pero con capas cuatro veces más pequeñas que las originales.

La estructura, por lo tanto, quedaría de la misma manera pero usando dos capas densas de 512 neuronas cada una y una capa densa final de 8. Para no tener que estar modificando el modelo original cada vez que haga falta es mucho más sencillo crear un modelo nuevo con $Keras$ y añadir todas las capas necesarias.

\begin{python}
def build_dense_layers():
    return [
        Flatten(),
        Dense(512, activation='relu'),
        Dense(512, activation='relu'),
        Dense(8, activation='softmax')
    ]

dense_model = keras.models.Sequential(build_dense_layers())
\end{python}

Un paso importante a la hora de construir el modelo personalizado es tener en cuenta que las salidas de las redes convolucionales poseen tres dimensiones ($ancho \times alto \times filtros$), mientras que la redes neuronales densas poseen solo una dimensión. Es necesario convertir la salida de estas capas a una entrada permitida. \textit{Keras} ya ofrece esta posibilidad usando una capa abstracta llamada \textit{Flatten}.

\begin{python}
# Entrenar la red
dense_model.compile(...)
dense_model.fit(...)
# Evaluar el modelo sobre el conjunto de test
dense_model.evaluate(test_feat, test_labels)
>>> [1.27161316430079874, 0.73067862714508579]
\end{python}

Los resultados obtenidos son ligeramente mejores que los anteriores, sin embargo no es aquí donde está toda la mejora. El entrenamiento de este modelo ha sido un 80\% más rápido que el anterior (19.5 segundos el primero, 3.9 segundos el último). Esto no solo ha permitido entrenar la red durante más tiempo, sino que en el futuro hará posible entrenar sobre mayores cantidades de datos sin aumentar el tiempo de entrenamiento.

\subsection{Mejorar el modelo}

La salida de Keras al entrenar el modelo ofrece información de lo que está sucediendo. Este es un ejemplo de la salida del entrenamiento del modelo anterior.

\begin{python}
Epoch 5/7
loss: 1.406 - acc: 0.621 - val_loss: 2.0197 - val_acc: 1.861
Epoch 6/7
loss: 0.992 - acc: 0.674 - val_loss: 1.4356 - val_acc: 1.001
Epoch 7/7
loss: 0.872 - acc: 0.761 - val_loss: 1.2772 - val_acc: 0.695
\end{python}

Los valores mostrados indican los resultados de la evaluación del modelo sobre el conjunto de entrenamiento y el de validación, respectivamente. Esto es mostrado para cada uno de los pasos. Se puede observar que el modelo funciona mucho mejor en el conjunto de entrenamiento que en el de validación.

Cuando un modelo tiene demasiados parámetros y ha sido entrenado durante demasiado tiempo aprende a clasificar los ejemplos con los que entrena usando información específica de cada uno de ellos en vez de generalizar. Esto se conoce como \textbf{sobreajuste} (\textit{overfitting}).

Los datos del modelo anterior indican que puede existir un sobreajuste, por lo que se va a intentar tomar medidas para arreglarlo.

\subsection{Dropout}

Una de las características de VGG y otras redes convolucionales es el uso de \textit{dropout} para reducir el sobreajuste de los modelos entrenados. El \textit{dropout} consiste en una capa que se aplica después de las capas de activación. Esta capa convierte activaciones aleatorias a 0, eliminando la información transportada (Figura \ref{dropout-net})

\begin{figure}
    \caption{A la izquierda una red neuronal estándar con dos capas ocultas. A la derecha la misma red aplicando un \textit{dropout} en cada una de las capas ocultas. Las neuronas tachadas han perdido su activación.}
\label{dropout-net}
  \makebox[\textwidth]{\includegraphics[width=0.7\linewidth]{dropout-net}}
\end{figure}



En un principio parecería que esto perjudica al modelo, pero al eliminar algunos de los pesos el modelo evita centrarse en características individuales de cada ejemplo de clasificación, obligándolo a generalizar más rápido \parencite{dropout}.

Un pequeño experimento en un modelo más avanzado (VGG con \textit{batch normalization} y aumento de datos) permite ver cómo afecta el \textit{dropout} a la puntuación final. En la figura \ref{dropout} se puede observar que la mejor puntuación se alcanza eliminando el 45 \% de las activaciones, consiguiendo una mejora de un 25 \% sobre el modelo que usa todas las activaciones.

\begin{figure}
    \caption{Evolución de la puntuación de un modelo usando \textit{dropout}}
\label{dropout}
  \makebox[\textwidth]{\includegraphics[width=0.7\linewidth]{dropout}}
\end{figure}


También se ve que el modelo empieza a perder eficacia a partir del 70 \% de \textit{dropout}, empeorando el modelo original. Esto significa que el modelo es capaz de generalizar con información útil incluso cuando solo posee el 30 \% de las activaciones.

Por otra parte, también es importante pensar dónde se pierden las activaciones. Por un lado, perder demasiadas activaciones en la entrada sería el equivalente a trabajar sin esos ejemplos. Por otro, perderlos en la salida sería aumentar demasiado el error al clasificar, al no tener tanta capacidad de apoyo en las otras capas.  La idea aplicada aquí es distribuir el \textit{dropout} disminuyéndolo en la entrada y la salida y haciendo que tenga su punto máximo en el centro de la red.

La definición de la red anterior quedaría de la siguiente manera:

\begin{python}
def build_dense_layers(p):
    return [
        Flatten(),
        Dropout(p/4),  # Aplicar 1/4 del dropout definido (p)
        Dense(512, activation='relu'),
        Dropout(p),
        Dense(512, activation='relu'),
        Dropout(p/2),  # Aplicar la mitad del dropout definido
        Dense(8, activation='softmax'),
    ]
dense_model = keras.models.Sequential(build_dense_layers(0.45))
\end{python}

Los resultados obtenidos con esta configuracion no mejoran los resultados de la red anterior, pero como se ve en la figura \ref{dropout} sí que lo hará a posteriori, cuando se le apliquen otro tipo de modificaciones al modelo. Modelos como VGG ya incluyen \textit{dropout} (solo en las redes densas), por lo que al hacer fine-tuning con una red personalizada es necesario usarlo para mantener los resultados originales.

\subsection{Normalización por lotes}

En muchos campos del aprendizaje automático es habitual normalizar las entradas de los modelos. Normalizar un conjunto de entradas hace que todas estén en la misma escala. Si existe una entrada mucho mayor o menor que el resto esta puede hacer que el entrenamiento arrastre un error mayor del necesario, produciendo inestabilidad y dificultando la convergencia del modelo.

El caso de las redes neuronales no es una excepción. Una entrada con un valor demasiado grande puede llevar a tener un peso determinado demasiado grande para contrarrestarla.

Una normalización estándar en aprendizaje automático es restar de cada entrada el valor medio del conjunto de datos y luego dividirlo por la desviación estándar del mismo. Esto aún presenta problemas para casos como el de este proyecto donde se usa el gradiente del descenso estocástico (cita a su definición en basicos:ann).

Los modelos principales citados en este trabajo, \parencite{krizhevsky2012imagenet} y \parencite{simonyan}, usan sus propias técnicas de normalización, aparte de usar activaciones \textit{ReLu} (sección \ref{sec:relu}) que son menos sensibles a los datos de entrada sin normalizar. Sin embargo, en 2015 se presenta una técnica muy interesante de normalización que cuadra muy bien con el gradiente del descenso estocástico: la normalización por lotes (o \textit{batch normalization}) \parencite{batch_normalization}.

La diferencia con una normalización estándar es que tras normalizar las entradas de una capa, se multiplican por un parámetro aleatorio, y luego se suma otro, cambiando así la desviación estándar y la media de la entrada. Estos cuatro parámetros (los dos parámetros nuevos, la desviación estándar y la media) se hacen entonces entrenables como pesos del modelo.

Los modelos actuales que usan esta normalización por lotes consiguen la misma precisión que los modelos sin ella usando catorce veces menos pasos de entrenamiento \parencite{batch_normalization}.

Para aplicar esta normalización por lotes al modelo que se está entrenando hay que añadir las capas de normalización por lotes (también disponibles en Keras) al modelo denso:

\begin{python}
def build_dense_layers(p):
    return [
        BatchNormalization(axis=1),
        Flatten(),
        Dropout(p/4),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(p),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(p/2),
        Dense(8, activation='softmax'),
    ]
dense_model = keras.models.Sequential(build_dense_layers(0.45))
\end{python}

Esto no es suficiente. A diferencia del \textit{dropout}, la normalización por lotes sí que es capaz de mejorar la red convolucional del modelo preentrenado. Gracias a la popularidad de esta técnica y de los modelos preentrenados que se usan en este trabajo, ya existen entrenamientos del modelo VGG original usando normalización por lotes, ahorrando la necesidad de entrenar una red tan grande. Una descripción de esto entrenamientos se puede encontrar en \parencite{pretrained_with_bn}.

En el caso de este trabajo se ha adaptado bajo la clase $Vgg16BN$. Al cambiar la red convolucional original hay que volver a transformar el conjunto de datos que se usaba para obtener los resultados de aplicar los filtros actualizados sobre las imágenes de entrada.

\begin{python}
# Carga del modelo VGG16 con normalizacion por lotes
from vgg16bn import Vgg16BN
import utils
model = Vgg16BN()
conv_model, _ = utils.split_at(model, MaxPooling2D)

# Cargamos los diferentes conjuntos de datos
train, train_labels = get_data(path + 'train')
valid, valid_labels = get_data(path + 'valid')
test, test_labels = get_data(path + 'test')

# Convertir dataset a features mediante la red convolucional
train_feat = conv_model.predict(train)
valid_feat = conv_model.predict(valid)
test_feat = conv_model.predict(test)

# Entrenar la red
dense_model = keras.models.Sequential(build_dense_layers(0.45))
dense_model.compile(...)
dense_model.fit(...)
# Evaluar el modelo sobre el conjunto de test
dense_model.evaluate(test_feat, test_labels)
>>> [0.16719577019912478, 0.9611234577100389]
\end{python}

Como se puede apreciar, los dos últimos cambios han supuesto una mejora considerable sobre el modelo anterior, clasificando correctamente el 96\% de los ejemplos del conjunto de test.

TODO: poner aqui la clasificacion de kaggle de este modelo, que es uno de los presentados.


\subsection{Aumento de datos}

Uno de los principales obstáculos de este problema es el reducido tamaño del conjunto de datos de entrenamiento. El tener pocos ejemplos sobre los que entrenar hace que al modelo le cueste generalizar sobre los ejemplos disponibles, produciéndose un sobreajuste.

Uno de los métodos que se usan para tratar de reducir el sobreajuste del modelo es el llamado aumento de datos (\textit{data augmentation}). Consiste en generar imágenes de entrenamiento adicionales a partir de las ya disponibles, rotando la imagen original, aumentándola, cambiando ligeramente el color, etc.

En el caso de nuestro problema esta técnica es especialmente interesante, ya que las cámaras de los barcos están fijas apuntando siempre a la misma zona. En las fotos sobre los peces capturados, estos siempre suelen estar con la cabeza apuntando en dirección contraria al agua. Ello hace que la red reconozca más fácilmente peces dispuestos en una determinada dirección pero costándole más trabajo para el resto de direcciones. Si las redes convolucionales ayudaban a detectar determinados patrones en cualquier punto de la imagen, el aumento de datos hace lo propio de otras variantes como la rotación, el tamaño o el filtro de color dado por el ambiente.

Un ejemplo de aumento de datos se muestra en la figura \ref{aug-original}, que contiene cuatro imágenes obtenidas al reescalar de distintas maneras la imagen original de la figura \ref{aug-original}.

\begin{figure}
    \caption{Imagen del conjunto de datos}
\label{aug-original}
  \makebox[\textwidth]{\includegraphics[width=0.7\linewidth]{aug-original}}
\end{figure}

\begin{figure}
    \caption{Cuatro aumentos diferentes de la imagen, reescalada a $224\times224$ píxeles}
\label{augmentations}
  \makebox[\textwidth]{\includegraphics[width=\linewidth]{augmentations}}
\end{figure}

Para trabajar con aumentos de datos, \textit{Keras} proporciona una serie de opciones de transformación dentro de la utilidad para importar conjuntos de datos. Al definir dichas opciones, como por ejemplo rotación, zoom o volteo horizontal, Keras generará nuevas imágenes a la vez que carga el conjunto de datos en memoria.

Si es necesario conocer cómo se aplican las transformaciones a las diferentes imágenes, es posible especificar un directorio de salida del generador de imágenes, de tal manera que los resultados de las distintas transformaciones se guarden en directorios particulares.

\begin{python}
# Crear el generador de imagenes
image_generator = image.ImageDataGenerator(
    rotation_range=30,
    horizontal_flip=True,
    zoom_range=0.4,
)

batches = img_generator.flow_from_directory(
    path+'test_aug',
    target_size=(224,224),
    class_mode='categorical',
    shuffle=False,
    batch_size=4,
    save_to_dir=path+'augmentation',
)

# Iterar sobre batches para extraer los datos
# En este caso se sacan 4 veces la cantidad de datos originales
data = [batch.next() for _ in range(batches.nb_samples * 4)]
\end{python}

Ahora no es necesario modificar el modelo, solo reentrenarlo con los nuevos datos. Como ha ocurrido antes, al cambiar el conjunto de datos hay que transformarlo pasándolo por la red convolucional, usando en este caso la última versión con normalización por lotes.

Lo interesante de ver en este caso es la reducción sobreajuste que se ha obtenido.

\begin{python}
Epoch 5/7
loss: 0.0768 - acc: 0.9817 - val_loss: 0.1142 - val_acc: 0.9538
Epoch 6/7
loss: 0.0639 - acc: 0.9895 - val_loss: 0.1212 - val_acc: 0.9334
Epoch 7/7
loss: 0.0708 - acc: 0.9860 - val_loss: 0.1139 - val_acc: 0.9520

dense_model.evaluate(test_feat, test_labels)
>>> [0.15222558558899649, 0.96723868954758185]
\end{python}

El modelo, que ahora ha predicho correctamente casi un 97\% de las imágenes del conjunto de test, ha aumentado ligeramente la puntuación de pérdida logarítmica. Hay que tener en cuenta que a medida que esta puntuación tiende a cero, es más difícil disminuirla, ya que corresponde cada vez más a un modelo casi perfecto.

Cuando hablamos de un modelo casi perfecto, nos referimos respecto al conjunto de test elegido. En este caso, debido al pequeño tamaño del conjunto de test, la perfección del modelo estará lejos de la perfección del modelo real que va a ser probado en el reto.


\section{Modelo convolucional}

Hasta ahora se ha seguido siempre el mismo esquema: una vez transformado el conjunto de datos entrenamos una red densa (completamente conectada) de tres capas. Además, una parte del preprocesamiento de las imágenes consiste en redimensionarlas a un tamaño más manejable, de $224\times224$ píxeles.

Una de las ventajas de las redes convolucionales es que permite aumentar el tamaño de las imágenes elegidas sin que por ello se incremente la complejidad de la red, ya que el número de pesos no cambiaría, solo el tamaño de la salida. Sin embargo, la complejidad de la red neuronal estándar que hemos colocado tras la red convolucional sí que vería incrementarse muchísimo su complejidad con el cambio de tamaño de las imágenes.

Para intentar aprovechar esta ventaja que ofrecen las redes convolucionales frente a las redes convolucionales clásicas, se puede intentar trabajar usando redes convolucionales en todo el proceso, clasificando al final con una única capa de activación clásica.

Aquí se estaría desarrollando una idea que se comentaba al principio de este trabajo, el conseguir ocho filtros cuya salida sea la probabilidad de que en la zona de la imagen esté apareciendo el pez de la clase elegida.

\begin{python}
def build_conv_layers():
    return [
        BatchNormalization(axis=1,
            input_shape=conv_layers[-1].output_shape[1:]
        ),
        Convolution2D(128, 3, 3, activation='relu', border_mode='same'),
        BatchNormalization(axis=1),
        Convolution2D(128, 3, 3, activation='relu', border_mode='same'),
        BatchNormalization(axis=1),
        Convolution2D(128, 3, 3, activation='relu', border_mode='same'),
        BatchNormalization(axis=1),
        Convolution2D(8, 3, 3, border_mode='same'),

        # Output layer
        GlobalAveragePooling2D(),
        Activation('softmax'),
    ]
\end{python}

Se usarán inicialmente tres capas de convoluciones, con 128 filtros cada una. La combinación de estos 128 filtros permitirá capturar la combinatoria de caracterísicas que representa la red convolucional preentrenada. Al final se añade una última capa de convolución con 8 filtros, el mismo número de categorías en las que clasificar.

Las imágenes resultantes de estos filtros se transforman a una sola entrada mediante una capa \textit{GlobalAveragePooling2D}. Esta capa funciona como las capas POOL de la red convolucional, pero usando la media del total de los píxeles de la imagen, generando una salida de un valor por filtro que pueda ser entendida por la capa de activación.

De nuevo habrá que transformar el conjunto de datos por medio de la red convolucional preentrenada, ya que se va a usar un tamaño de imagen de $360\times640$.

Aunque, como se ha comentado, la red convolucional acepta un aumento del tamaño de la imagen sin aumentar exponencialmente el tiempo de entrenamiento, el entrenamiento de una red convolucional es mucho más costoso que el de una red clásica. Esto se debe a que hay que actualizar $n\timesn$ pesos por cada píxel de cada imagen. En este caso el entrenamiento de la red completamente convolucional ha tardado 24 veces más que el entrenamiento de la red densa del capítulo anterior.

\begin{python}
conv_model.evaluate(test_feat, test_labels)
>>> [0.14251796898005481, 0.973238231345685]
\end{python}

Al evaluar el modelo se ve que consigue una pequeña mejora respecto al modelo denso. Pero el aumento de puntuación no es la única ventaja de este modelo.

\subsection{Visualización del modelo}

El trabajar en todo momento (salvo en la última capa= con redes convolucionales, implica que los pesos son siempre filtros que se aplican a imágenes. Se pretende entonces conseguir filtros que resalten la existencia en cada zona de la imagen de un pez de una determinada clase.

Se pueden usar los diferentes valores de los filtros para construir un mapa de calor para la categoría que le corresponde. Un mapa de calor es una representación gráfica de los valores de una matriz bidimensional. Generalmente valores pequeños se asocian a colores más claros,como el azul o el verde, y valores mayores a colores más cálidos, como el rojo o el morado.  Si se redimensiona el filtro (ya que ha sido reducido por las capas de MaxPooling2D) al tamaño de la imagen original se puede ver cómo encuentra el pez a clasificar en cada imagen, tal y como se muestra por ejemplo en la figura 4.8.

\begin{figure}
    \caption{Una imagen correspondiente a la clase ALB: \textit{Thunnus alalunga}}
\label{fc-fish}
  \makebox[\textwidth]{\includegraphics[width=\linewidth]{fc-fish}}
\end{figure}

\begin{figure}
    \caption{Aplicación del mapa de calor reescalado a la imagen \ref{fc-fish}}
\label{fc-heatmap}
  \makebox[\textwidth]{\includegraphics[width=\linewidth]{fc-heatmap}}
\end{figure}

Esto no solo permite comprobar que el modelo funciona correctamente, sino también buscar los ejemplos que peor clasifica y analizar qué está señalando el mapa de calor correspondiente.

Un ejemplo claro de cómo se puede usar esta técnica de visualización para detectar los errores en el modelo es el intento de clasificación un atún de cola amarilla (YFT: \textit{Thunnus albacares}), en la imagen \ref{yft}.

Una característica de los atunes de cola amarilla es su banda de tonos amarillos en el lomo, característica que el modelo podría haber aprendido gracias a los ejemplos. Sin embargo la imagen \ref{yft} muestra el atún visto desde abajo, con las aletas abiertas. Esto esconde una de las principales características del atún, siendo difícil clasificarlo incluso para un humano no familiarizado con atunes.

Al mostrar el mapa de calor del YFT en la imagen \ref{yft-heatmap} se puede apreciar que la hipótesis de que el modelo ha aprendido a detectar los atunes de cola amarilla a través del color podría ser cierta, ya que una de las partes más importantes de la foto para el filtro son los pantalones del pescador, de un color amarillo fuerte.

Esto pertime tomar ciertas decisiones sobre el entrenamiento del modelo. Por ejemplo, si se ve que la fijación del modelo con el color es un error y es preferible que use otras cosas como formas o texturas, se puede eliminar el color de las imágenes o aumentar los datos usando transformaciones de color que hagan especial hincapié en la varianza del color amarillo.

\begin{figure}
    \caption{Una imagen correspondiente a la clase ALB: \textit{Thunnus alalunga}}
\label{yft}
  \makebox[\textwidth]{\includegraphics[width=\linewidth]{yft}}
\end{figure}

\begin{figure}
    \caption{Aplicación del mapa de calor reescalado a la imagen \ref{fc-fish}}
\label{yft-heatmap}
  \makebox[\textwidth]{\includegraphics[width=\linewidth]{yft-heatmap}}
\end{figure}

\section{Modelo con entrada múltiple}

Un examen detallado del conjunto de datos de entrada nos permite detectar determinadas propiedades de las imágenes cuyo uso podría permitir una mejora de los modelos. Por ejemplo, las imágenes están tomadas en diferentes barcos, con diferentes modelos de cámaras. No todas las cámaras generan imágenes del mismo tamaño, por lo que se podría averiguar el barco del que se ha tomado la fotografía por el tamaño de la imagen.

Ya que cada barco suele pescar diferentes tipos de peces con mayor o menor probabilidad, el conocer el tipo de barco puede ser aprovechado por el modelo para balancear las posibilidades de cada categoría dependiendo del tipo de pesca que suela llevar.

Obviamente la idea no es hacer todo esto a mano, sino entrenar un modelo que tenga en cuenta tanto las imágenes como otra información válida. En este caso se va a elegir el modelo denso, con la diferencia que la última capa tendrá dos entradas: la salida de las capas ocultas de la red neuronal y el tamaño de la imagen, codificado en \textit{onehot}.

TODO Hacer un esquema de las arquitectura de la red.

\begin{python}
def build_dense_layers(p):
    # Usamos las entradas de la red convolucional, como antes
    inp = Input(conv_layers[-1].output_shape[1:])
    x = MaxPooling2D()(inp)
    x = BatchNormalization(axis=1)
    x = Flatten()
    x = Dropout(p/4)
    x = Dense(512, activation='relu')
    x = BatchNormalization()
    x = Dropout(p)
    x = Dense(512, activation='relu')
    x = BatchNormalization()
    x = Dropout(p/2)

    # Usamos las entradas de las dimensiones de cada elemento
    size_inputs = Input(len(sizes))
    bn_inputs = BatchNormalization()(size_inputs)

    # Unimos ambas entradas en una última capa
    x = merge([x, bn_inp], 'concat']
    x = Dense(8, activation='softmax')
    return x

dense_model = keras.models.Model(
    [inp, size_inputs],  # El modelo ahora tiene multi entrada
    build_dense_layers(0.45)
)
\end{python}

Tras construir el modelo con las entradas múltiples y entrenarlo sobre los dos conjuntos de entrenamiento (las imágenes y los tamaños de las imágenes) vemos la evaluación del modelo.

\begin{python}
dense_model.evaluate(test_feat, test_labels)
>>> [0.1725, 0.9413]
\end{python}

El modelo ha sacado una evaluación peor que modelos anteriores, incluso usando un modelo que ya se conocía con una evaluación superior.

Aunque la idea es buena, lo que está haciendo por debajo es clasificar los diferentes barcos en base a los tamaños de sus imágenes. Debido a la amplia diferencia de las imágenes de los diferentes tipos de barcos, ya sea por los colores, iluminación, objetos, etc, el modelo denso probablemente ya esté teniendo en cuenta de qué barco es cada imagen. Por lo tanto lo único que va a añadir el introducir estos datos es complejidad del modelo, haciendo más difícil converger a un modelo bueno.

\subsubsection{Fuga de datos}

Esta idea de usar datos diferentes del conjunto de datos para clasificar es algo muy usado en las competiciones de modelos predictivos. No solo funciona el tamaño de la foto, sino a veces las fotos tienen datos adicionales, como información GPS, hora de creación o modelo de cámara.

No todos estos datos son publicados de una manera voluntaria por comunidades de campeonatos, pero son útiles para conseguir una pequeña mejora en el modelo, siempre que el modelo puede aprovecharse de estos datos.

\section{Salida múltiple}

Como vimos anteriormente en la figura \ref{yft-heatmap} la red completamente convolucional es capaz de clasificar correctamente la imagen, un atún de cola amarilla, pero por las razones incorrectas: los pantalones del pescador. Un modelo que sea capaz de clasificar la imagen e identificar la localización del pez podría evitar este tipo de errores. Intuitivamente tiene sentido, ya que es lógico buscar primero si hay un pez en la imagen y luego distinguir a qué categoría pertenece.

Al usar técnicas de aprendizaje supervisado es necesario ejemplos resueltos para poder generar un modelo y el conjunto de datos del problema no posee ninguna información sobre la localización de los peces en la imagen.

Las reglas de la competición en Kaggle indican que es posible usar conjuntos de datos auxiliares generados por los usuarios, siempre que estos se hagan públicos y estén disponibles para todos los participantes. Afortunadamente un usuario ha publicado un conjunto de datos que contienen las coordenadas de los rectángulos de todos las imágenes del conjunto de entrenamiento.

El conjunto de datos consiste en un archivo \textit{json} con la siguiente estructura.

\begin{python}
{
    "annotations": [
        {
            "class": "rect",
            "height": 79.0,
            "width": 256.0,
            "x": 815.0,
            "y": 124.0
        }
    ],
    "class": "image",
    "filename": "img_07795.jpg"
},
\end{python}

Cada rectángulo se define mediante su altura (height), anchura (width) y las coordenadas del centro del rectángulo (x, y). Algunas imágenes contienen varios peces, por lo que su clave en el conjunto de datos contienen varios rectángulos.

Si pintamos uno de los rectángulos encima de la imagen a la que corresponde obtenemos la imagen de la figura \ref{box}, que captura correctamente la ubicación del pez.

Para usar esta información en el modelo vamos a usar un modelo de salida múltiple. Este modelo, dada una imagen, devolverá una posible clasificación de la imagen y al mismo tiempo generará las coordenadas de un rectángulo que contenga el pez. El modelo tendrá que usar las mismas características de cada imágen para devolver am


\begin{figure}
  \caption{Representación de un rectángulo del conjunto de datos auxiliar que ubica cada pez en su imagen}
\label{box}
  \makebox[\textwidth]{\includegraphics[width=0.5\linewidth]{box}}
\end{figure}

